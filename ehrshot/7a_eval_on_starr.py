"""
This file should be run after 7_eval_finetune.py. 
It loads the model checkpoints generated by that script, and evaluates them on the entire STARR test set.

Usage:
    python3 ../7a_eval_on_starr.py \
        --path_to_labels_dir '../../EHRSHOT_ASSETS/benchmark_starr' \
        --path_to_features_dir '../../EHRSHOT_ASSETS/features_starr' \
        --path_to_split_csv '../../EHRSHOT_ASSETS/splits_starr/person_id_map.csv' \
        --path_to_output_dir '../../EHRSHOT_ASSETS/results_starr_new' \
        --labeling_function guo_los \
        --shot_strat all \
        --models clmbr \
        --heads gbm,lr_lbfgs,rf
"""
import argparse
import json
import os
import pickle
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np
import collections
import pandas as pd
from tqdm import tqdm
import sklearn
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from torch.optim.lr_scheduler import LambdaLR
from loguru import logger
from sklearn.preprocessing import MaxAbsScaler
from utils import (
    LABELING_FUNCTION_2_PAPER_NAME,
    SHOT_STRATS,
    MODEL_2_INFO,
    get_labels_and_features, 
    process_chexpert_labels, 
    convert_multiclass_to_binary_labels,
    CHEXPERT_LABELS, 
    LR_PARAMS, 
    XGB_PARAMS, 
    RF_PARAMS,
    ProtoNetCLMBRClassifier, 
    get_patient_splits_by_idx
)
from sklearn.model_selection import GridSearchCV, PredefinedSplit
from scipy.sparse import issparse
import scipy
import lightgbm as lgb
import femr
import femr.datasets
import torch
from jaxtyping import Float
from femr.labelers import load_labeled_patients, LabeledPatients
from hf_ehr.utils import load_tokenizer_from_path, load_model_from_path
from hf_ehr.eval.ehrshot import CookbookModelWithClassificationHead


def calc_metrics(y_test: Float[np.ndarray, 'N'], 
                 y_test_proba: Float[np.ndarray, 'N'], 
                 test_patient_ids: List[int]) -> Dict[str, Dict[str, float]]:
    """Calculates AUROC, AUPRC, and Brier scores for train, val, and test sets.
        NOTE: Expects `y_train_proba`, `y_val_proba`, and `y_test_proba` to be the probability of the positive class.
    """

    metric_dict = {
        'auroc': metrics.roc_auc_score,
        'brier': metrics.brier_score_loss,
        'auprc': metrics.average_precision_score,
    }
    
    # Calculate metrics
    scores = {}
    for metric, func in metric_dict.items():
        scores[metric] = {}
        test_score = func(y_test, y_test_proba)

        logger.info(f"Test {metric} score:  {test_score}")

        test_set = sorted(list(set(test_patient_ids)))

        score_list = []
        for i in range(1000): # 1k bootstrap replicates
            sample = sklearn.utils.resample(test_set, random_state=i)
            counts = collections.Counter(sample)
            weights = np.zeros_like(test_patient_ids)

            for i, p in enumerate(test_patient_ids):
                weights[i] = counts[p]

            score_val = func(y_test, y_test_proba, sample_weight=weights)
            score_list.append(score_val)

        # 95% CI
        lower, upper = np.percentile(score_list, [2.5, 97.5])

        # Std
        std = np.std(score_list, ddof=1)

        scores[metric]['score'] = test_score
        scores[metric]['std'] = std
        scores[metric]['lower'] = lower
        scores[metric]['mean'] = np.mean(score_list)
        scores[metric]['upper'] = upper
    return scores

def eval_pytorch_model(X_test_timelines: np.ndarray,
                        y_test: np.ndarray,
                        model: torch.nn.Module, 
                        pad_token_id: int,
                        model_name: str, 
                        model_head: str, 
                        batch_size: int,
                        device: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    model.eval()
    y_test_proba = []
    with torch.no_grad():
        for batch_start in tqdm(range(0, X_test_timelines.shape[0], batch_size), desc=f'Inference: model={model_name[:15]} | head={model_head[:15]}', total=X_test_timelines.shape[0] // batch_size):
            X_batch = X_test_timelines[batch_start:batch_start+batch_size]
            input_ids = torch.tensor(X_batch, device=device)
            attention_mask = (input_ids != pad_token_id).int()
            batch = {
                'input_ids': input_ids,
                'attention_mask': attention_mask,
            }
            if "hyena" in model_name:
                batch.pop('attention_mask')
            y_test_proba.append(model.predict_proba(**batch)[:, 1].detach().cpu().numpy())
    y_test_proba = np.concatenate(y_test_proba) if len(y_test_proba) > 0 else np.array([])
    return y_test_proba

def run_finetune_evaluation(X_test: np.ndarray, 
                            y_test: np.ndarray,
                            X_test_timelines: np.ndarray,
                            model_name: str, 
                            model_head: str, 
                            path_to_ckpt: str,
                            batch_size: int = 4,
                            test_patient_ids: List[int] = None) -> Tuple[Any, Dict[str, Dict[str, float]], Dict[str, np.ndarray]]:
    logger.info(f"Test prevalence:  {np.mean(y_test)}")
    logger.info(f"Test pids:  {len(test_patient_ids)} | {len(y_test)} | {len(set(test_patient_ids))}")

    # Load tokenizer
    device: str = 'cuda'
    tokenizer = load_tokenizer_from_path(path_to_ckpt)
    pad_token_id: int = tokenizer.pad_token_id
    finetune_strat = model_head.split("_")[1] # "layers=n" or "full"
    embed_strat: str = [ x for x in model_name.split("_") if x.split(":")[0] == 'embed' ][0].split(":")[1] # "mean" or "last"

    # Load original model -- weights won't change
    orig_model = load_model_from_path(path_to_ckpt)
    orig_model = CookbookModelWithClassificationHead(orig_model, embed_strat, 2)
    # Load model that will get finetuned for each logreg_C -- weights will change
    model = load_model_from_path(path_to_ckpt)
    model = CookbookModelWithClassificationHead(model, embed_strat, 2)
    model: torch.nn.Module = torch.compile(model)  # type: ignore

    # Load best model
    model._orig_mod.load_state_dict(best_model_state_dict)
    model._orig_mod.get_params = lambda : best_hparams
 
    # Eval best model on train/val/test sets
    logger.info(f"Start | Evaling best model=`{model_name}` | head=`{model_head}` | logreg_C=`{C}`")
    y_test_proba = eval_pytorch_model(X_test_timelines, y_test, 
                                      model, pad_token_id, 
                                      model_name, model_head, batch_size, device)
    logger.info(f"Finish | Evaling best model=`{model_name}` | head=`{model_head}` | logreg_C=`{C}`")
        
    # Calculate AUROC, AUPRC, and Brier scores
    best_scores = calc_metrics(y_test, y_test_proba, test_patient_ids)
    best_metrics: Dict[str, np.ndarray] = { 'test' : y_test_proba }

    # Return best model -- if torch.compile() is used, then we need to unwrap it to return the original model
    logger.debug(f"Finish | Training {model_head} | replicate={replicate}")
    model.to('cpu')
    return model._orig_mod, best_scores, best_metrics

def run_frozen_feature_evaluation(X_test: np.ndarray, 
                                    y_test: np.ndarray, 
                                    test_patient_ids: List[int] = None,
                                    path_to_ckpt: str = None) -> Tuple[Any, Dict[str, float]]:
    logger.info(f"Test prevalence:  {np.mean(y_test)}")
    logger.info(f"Test pids:  {len(test_patient_ids)} | {len(y_test)} | {len(set(test_patient_ids))}")

    # Load sklearn model from .ckpt
    model = pickle.load(open(path_to_ckpt, 'rb'))
    logger.info(f"Best hparams: {model.get_params()}")
    
    breakpoint()
    
    # Calculate probabilistic preds
    y_test_proba: Float[np.ndarray, 'N'] = model.predict_proba(X_test)[:, 1]
    
    # Calculate AUROC, AUPRC, and Brier scores
    scores = calc_metrics(y_test, y_test_proba, test_patient_ids)
    return model, scores, { 'test' : y_test_proba }

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run EHRSHOT evaluation benchmark on a specific task.")
    parser.add_argument("--path_to_labels_dir", required=True, type=str, help="Path to directory containing saved labels")
    parser.add_argument("--path_to_features_dir", required=True, type=str, help="Path to directory containing saved features")
    parser.add_argument("--path_to_output_dir", required=True, type=str, help="Path to directory where results will be saved")
    parser.add_argument("--path_to_split_csv", required=True, type=str, help="Path to CSV of splits")
    parser.add_argument("--path_to_tokenized_timelines_dir", required=False, type=str, default=None, help="Path to directory containing tokenized timelines (if applicable)")
    parser.add_argument("--shot_strat", type=str, choices=SHOT_STRATS.keys(), help="What type of X-shot evaluation we are interested in.", required=True )
    parser.add_argument("--labeling_function", required=True, type=str, help="Labeling function for which we will create k-shot samples.", choices=LABELING_FUNCTION_2_PAPER_NAME.keys(), )
    parser.add_argument("--is_force_refresh", action='store_true', default=False, help="If set, then overwrite all outputs")
    parser.add_argument("--models", required=True, help="Comma separated list. If specified, then only consider models in this list, e.g. `clmbr,count`")
    parser.add_argument("--heads", default=None, help="Comma separated list. If specified, then only consider heads in this list, e.g. `finetune_layers=1,finetune_layers=2`")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size for finetuning. NOTE: This must be a small value (e.g. 4) for torch.compile() to correctly work")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    LABELING_FUNCTION: str = args.labeling_function
    VALID_HEADS: Optional[List[str]] = None if args.heads is None else args.heads.split(',')
    VALID_MODELS: List[str] = args.models.split(',')
    SHOT_STRAT: str = args.shot_strat
    BATCH_SIZE: int = args.batch_size
    PATH_TO_TOKENIZED_TIMELINES_DIR: Optional[str] = args.path_to_tokenized_timelines_dir
    IS_FORCE_REFRESH: bool = args.is_force_refresh
    PATH_TO_FEATURES_DIR: str = args.path_to_features_dir
    PATH_TO_LABELS_DIR: str = args.path_to_labels_dir
    PATH_TO_SPLIT_CSV: str = args.path_to_split_csv
    PATH_TO_LABELED_PATIENTS: str = os.path.join(PATH_TO_LABELS_DIR, LABELING_FUNCTION, 'labeled_patients.csv')
    PATH_TO_OUTPUT_DIR: str = args.path_to_output_dir
    PATH_TO_OUTPUT_FILE: str = os.path.join(PATH_TO_OUTPUT_DIR, LABELING_FUNCTION, f'{SHOT_STRAT}_results.csv')
    os.makedirs(os.path.dirname(PATH_TO_OUTPUT_FILE), exist_ok=True)
    assert BATCH_SIZE <= 4, f"You should only use a --batch_size <= 4 for optimal use of torch.compile()"

    # Determine which models to load
    logger.critical(f"Only running models: {VALID_MODELS}")
    logger.critical(f"Only running heads: {VALID_HEADS}")

    # Load all labeled patients
    labeled_patients: LabeledPatients = load_labeled_patients(PATH_TO_LABELED_PATIENTS)
    logger.info(f"Loading task {LABELING_FUNCTION} with {len(labeled_patients)} labeled patients.")
    
    # For each base model we are evaluating...
    for model in VALID_MODELS:
        model_base_name: str = model.split("-")[0] # count -> count; gpt-base-1024 -> gpt
        model_heads: List[str] = MODEL_2_INFO[model_base_name]['heads']

        # For each head we can add to the top of this model...
        for head in model_heads:
            if VALID_HEADS is not None and head not in VALID_HEADS:
                # Skip heads (if specified)
                continue
            
            if 'finetune' in head:
                assert PATH_TO_TOKENIZED_TIMELINES_DIR is not None, "Must specify `--path_to_tokenized_timelines_dir` for finetuning experiments"

            # Load labels/features for this task + model_head
            patient_ids, label_values, label_times, feature_matrixes = get_labels_and_features(labeled_patients, 
                                                                                               PATH_TO_FEATURES_DIR, 
                                                                                               PATH_TO_TOKENIZED_TIMELINES_DIR,
                                                                                               models_to_keep=[ model ])
            __, __, test_pids_idx = get_patient_splits_by_idx(PATH_TO_SPLIT_CSV, patient_ids)

            # Preprocess certain non-binary labels
            if LABELING_FUNCTION == "chexpert":
                label_values = process_chexpert_labels(label_values)
                sub_tasks: List[str] = CHEXPERT_LABELS
            elif LABELING_FUNCTION.startswith('lab_'):
            # Lab value is multi-class, convert to binary
                label_values = convert_multiclass_to_binary_labels(label_values, threshold=1)
                sub_tasks: List[str] = [LABELING_FUNCTION]
            else:
                # Binary classification
                sub_tasks: List[str] = [LABELING_FUNCTION]
                
            assert model in feature_matrixes, f"Feature matrix not found for `{model}`. Are you sure you have generated features for this model? If not, you'll need to rerun `generate_features.py` or `generate_clmbr_representations.py`."
            
            # Unpack each individual featurization we want to test
            if head.startswith('finetune'):
                assert torch.cuda.is_available(), "CUDA must be available to run PyTorch models."
                X_test_timelines: np.ndarray = feature_matrixes[model]['timelines'][test_pids_idx]
                X_test: np.ndarray = feature_matrixes[model]['frozen'][test_pids_idx]
            else:
                X_test_timelines = None
                X_test: np.ndarray = feature_matrixes[model]['frozen'][test_pids_idx]
            
            # Test labels
            y_test: np.ndarray = label_values[test_pids_idx]
            test_patient_ids = patient_ids[test_pids_idx]
            
            # Always just do k=-1
            k = -1
            replicate = 0
            
            # For each subtask in this task... 
            # NOTE: The "subtask" is just the same thing as LABELING_FUNCTION for all binary tasks.
            # But for Chexpert, there are multiple subtasks, which of each represents a binary subtask
            for sub_task_idx, sub_task in enumerate(sub_tasks):
                ############################
                # ! Do loading of previous results here so that we reload any results that
                # ! were generated in a concurrently running SLURM job
                # If results already exist, then append new results to existing file
                df_existing: Optional[pd.DataFrame] = None
                if os.path.exists(PATH_TO_OUTPUT_FILE):
                    logger.warning(f"Results already exist @ `{PATH_TO_OUTPUT_FILE}`.")
                    df_existing = pd.read_csv(PATH_TO_OUTPUT_FILE)
                ############################
                
                # Check if results already exist for this model/head/shot_strat in `results.csv`
                if df_existing is not None:
                    existing_rows: pd.DataFrame = df_existing[
                        (df_existing['labeling_function'] == LABELING_FUNCTION) 
                        & (df_existing['sub_task'] == sub_task) 
                        & (df_existing['model'] == model) 
                        & (df_existing['head'] == head)
                        & (df_existing['k'] == k)
                        & (df_existing['replicate'] == replicate)
                    ]
                    if existing_rows.shape[0] > 0:
                        # Overwrite
                        if IS_FORCE_REFRESH:
                            logger.warning(f"Results ALREADY exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`.\nOVERWRITING these rows because `is_force_refresh` is TRUE.")
                        else:
                            logger.warning(f"Results ALREADY exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`.\nSKIPPING this combination because `is_force_refresh` is FALSE.")
                            continue
                    else:
                        # Append
                        logger.warning(f"Results DO NOT exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`. Appending to this CSV.")
        
                logger.success(f"Model: {model} | Head: {head} | Task: {sub_task} | k: {k} | replicate: {replicate}")
                
                # Test labels
                y_test_k: np.ndarray = np.array(y_test)

                # CheXpert adjustment
                if LABELING_FUNCTION == 'chexpert':
                    y_test_k = y_test[:, sub_task_idx]

                # Get path to saved model
                path_to_model_dir: str = os.path.abspath(os.path.join(PATH_TO_FEATURES_DIR, '../models', model, head, f'subtask={sub_task}', f'k={k}'))
            
                # Run eval
                if head.startswith('finetune'):
                    assert os.path.exists(path_to_model_dir), f"Path to .ckpt directory for model={model},head={head} does not exist: `{path_to_model_dir}`"
                    path_to_ckpt: str = os.path.join(path_to_model_dir, [ x for x in os.listdir(path_to_model_dir) if x.endswith('.ckpt') ][0])
                    logger.info(f"Loaded model `{model}` from .ckpt at: `{path_to_ckpt}`")
                    best_model, scores, preds_proba = run_finetune_evaluation(X_test, y_test_k, X_test_timelines,
                                                                                model_name=model, model_head=head, path_to_ckpt=path_to_ckpt, batch_size=BATCH_SIZE,
                                                                                test_patient_ids=test_patient_ids)
                else:
                    path_to_ckpt: str = os.path.join(path_to_model_dir, [ x for x in os.listdir(path_to_model_dir) if x.endswith('.pkl') ][0])
                    best_model, scores, preds_proba = run_frozen_feature_evaluation(X_test, y_test_k, X_test_timelines, 
                                                                                    test_patient_ids=test_patient_ids,
                                                                                    path_to_ckpt=path_to_ckpt)


                ############################
                # Reload to ensure we have most updated version of file
                if os.path.exists(PATH_TO_OUTPUT_FILE):
                    df_existing = pd.read_csv(PATH_TO_OUTPUT_FILE)
                    results += df_existing.to_dict(orient='records')
                ############################

                # Save results to CSV after each (model, head, sub_task, k, replicate) is calculated
                logger.critical(f"Saving results for {model} + {head} + k={k} + replicate={replicate} to: {PATH_TO_OUTPUT_FILE}")
                df: pd.DataFrame = pd.DataFrame(results)
                logger.critical(f"Added {df.shape[0] - (df_existing.shape[0] if df_existing is not None else 0)} rows")
                df.to_csv(PATH_TO_OUTPUT_FILE, index=False)
    logger.success("Done!")
